{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "\n",
    "Date: 30.03.2019\n",
    "\n",
    "Environment: Python 3.6.8 and Anaconda 4.6.7 (64-bit)\n",
    "\n",
    "Libraries used:\n",
    "* re 2.2.1 (for regular expression, included in Anaconda Python 3.6) \n",
    "* json 2.6  (for working with JSON data, included in Anaconda Python 3.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>body {\n",
       "    margin: 0;\n",
       "    font-family: Helvetica;\n",
       "}\n",
       "table.dataframe {\n",
       "    border-collapse: collapse;\n",
       "    border: none;\n",
       "}\n",
       "table.dataframe tr {\n",
       "    border: none;\n",
       "}\n",
       "table.dataframe td, table.dataframe th {\n",
       "    margin: 0;\n",
       "    border: 1px solid white;\n",
       "    padding-left: 0.25em;\n",
       "    padding-right: 0.25em;\n",
       "}\n",
       "table.dataframe th:not(:empty) {\n",
       "    background-color: #fec;\n",
       "    text-align: left;\n",
       "    font-weight: normal;\n",
       "}\n",
       "table.dataframe tr:nth-child(2) th:empty {\n",
       "    border-left: none;\n",
       "    border-right: 1px dashed #888;\n",
       "}\n",
       "table.dataframe td {\n",
       "    border: 2px solid #ccf;\n",
       "    background-color: #f4f4ff;\n",
       "}\n",
       "\n",
       "h2 {\n",
       "    color: white;\n",
       "    background-color: rgb(20, 167, 142);\n",
       "    padding: 0.5em;\n",
       "}\n",
       "h3 {\n",
       "    color: white;\n",
       "    background-color: rgb(20, 167, 142);\n",
       "    padding: 0.5em;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "css = open('style/style-table.css').read() + open('style/style-notebook.css').read()\n",
    "HTML('<style>{}</style>'.format(css))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "This report focuses on the logics and the implementation of parsing text files and text pre-processing in this Unit Info task using Python. The main purpose of this report is to provide information about the methodology and process to solve this problem.\n",
    "\n",
    ">The required tasks are the following:\n",
    ">1. Extract the data for each unit from `.txt` file.\n",
    ">2. Transform the data to the `.json` file.\n",
    ">3. Transform the data to the `.xml` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">The processes implemented in this report are :\n",
    ">1. Understand the format and hierarchical structure of output file.\n",
    ">2. Loading input data and split it into different unit blocks.\n",
    ">3. Extract data for each keyword and store it into correct format.\n",
    ">4. Write into `.json` and `.xml` file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Understand the format and hierarchical structure of output file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Read output json file \n",
    "> * Explore the structure of the output\n",
    "> * Use output file to get the right type and format of data container \n",
    "> * Visualize the whole structure and logic of storing different info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-level is a dict with one key: dict_keys(['units'])\n",
      "2nd-level and 3-rd level are:  <class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "with open('style/test_output.json', 'r') as f:\n",
    "    test_output = json.load(f)\n",
    "print(f'Top-level is a dict with one key: {test_output.keys()}')\n",
    "print(f\"2nd-level and 3-rd level are:  {type(test_output['units']['unit'][0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'@id': {\"<class 'str'>\"},\n",
       " 'title': {\"<class 'str'>\"},\n",
       " 'synopsis': {\"<class 'str'>\"},\n",
       " 'pre_requistics': {\"<class 'dict'>\", \"<class 'str'>\"},\n",
       " 'prohibisions': {\"<class 'dict'>\", \"<class 'str'>\"},\n",
       " 'requirements': {\"<class 'dict'>\", \"<class 'str'>\"},\n",
       " 'outcomes': {\"<class 'dict'>\"},\n",
       " 'chief_examiners': {\"<class 'dict'>\", \"<class 'str'>\"}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# explore the type of data container and string format for each keyword\n",
    "check_type_dict = {}\n",
    "for i in range(100):\n",
    "    for key,value in test_output['units']['unit'][i].items():\n",
    "        if key in check_type_dict:\n",
    "            check_type_dict[key].append(str(type(value))) \n",
    "        else:\n",
    "            check_type_dict[key]=[]\n",
    "            \n",
    "for key,value in check_type_dict.items():\n",
    "    check_type_dict[key] = set(check_type_dict.get(key))\n",
    "check_type_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"style/img_2_json_structure.png\" height = \"500\" width = \"900\" style=\"float: left;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Loading input data and split it into different unit blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * As a first step, the `.txt file` will be loaded as a string containing all info.\n",
    "> * After explore the test input file and the real data file, we found that the **<font color=blue>Navigation links</font>** part can be used as seperators to split the whole data string into different blocks.\n",
    "> * We use **<font color=blue>nav</font>** and **<font color=blue><\\nav></font>** to find the section of navigation links as seperators\n",
    "> * After splitting, we drop the first blank one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('style/30086434.txt', 'r')\n",
    "data_str = f.read()  \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 400 nav section between each unit block.\n",
      "The # of unique section is 1.\n",
      "Section of navigation: \n",
      "------\n",
      " <nav class=\"breadcrumbs mobile-hidden\" id=\"breadcrumbs\">\n",
      "<p class=\"visuallyhidden\" id=\"breadcrumb__label\">You are here:</p>\n",
      "<ul aria-labelledby=\"breadcrumb__label\" class=\"breadcrumbs__list\">\n",
      "<li class=\"breadcrumbs__item home\">\n",
      "<a class=\"breadcrumbs__link\" href=\"https://www.monash.edu/\">Home</a>\n",
      "<span aria-hidden=\"true\" class=\"breadcrumbs__divider\">|</span>\n",
      "</li>\n",
      "<li class=\"breadcrumbs__item\">\n",
      "<a class=\"breadcrumbs__link\" href=\"https://www.monash.edu/study\">Study</a>\n",
      "<span aria-hidden=\"true\" class=\"breadcrumbs__divider\">|</span>\n",
      "</li>\n",
      "<li class=\"breadcrumbs__item\">\n",
      "<a class=\"breadcrumbs__link\" href=\"/pubs/2019handbooks/\">2019 Handbooks</a>\n",
      "<span aria-hidden=\"true\" class=\"breadcrumbs__divider\">|</span>\n",
      "</li>\n",
      "<li class=\"breadcrumbs__item breadcrumbs__current\"><a class=\"breadcrumbs__link\" href=\"/pubs/2019handbooks/units\">Units</a></li>\n",
      "</ul>\n",
      "</nav>\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "nav_pattern = re.compile(r'<nav.*?</nav>',re.S)\n",
    "nav_str_list = re.findall(nav_pattern,data_str)\n",
    "# Check whether the nav string is unique for each unit\n",
    "print(f'There are {len(nav_str_list)} nav section between each unit block.')\n",
    "print(f'The # of unique section is {len(set(nav_str_list))}.')\n",
    "print(f'Section of navigation: \\n------\\n {nav_str_list[0]}\\n------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 400 unit blocks in the whole dataset.\n"
     ]
    }
   ],
   "source": [
    "# Split the str into different unit blocks and drop the first blank one\n",
    "nav_str = nav_str_list[0]\n",
    "unit_block_split = data_str.split(nav_str)\n",
    "unit_block_list = unit_block_split[1:]\n",
    "print(f'There are {len(unit_block_list)} unit blocks in the whole dataset.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Extract info for each keyword and store into correct format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For each keyword, the function will complete following sub-tasks:\n",
    "1. Extract data\n",
    "2. Write into correct data container i.e.dict or list or string for `.json output`\n",
    "3. Write into a string for the `.xml output`\n",
    "  * Because some special characters cannot directly expressed in XML-format text.<br> \n",
    "    So they are expressed in a special way using XML's escape charter &.<br> \n",
    "    XML representations for **<font color=blue>&, >, <</font>**  --->  **<font color=blue>&amp, &gt, &lt</font>**.<br> \n",
    "    Therefore, we need to use re.sub to do the replacement.<br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to replace the special character for json data\n",
    "def sub_xml_cha(s):\n",
    "    s = re.sub('&amp;','&',s)\n",
    "    s = re.sub('&gt;','>',s)\n",
    "    s = re.sub('&lt;','<',s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to transform into special character for xml file\n",
    "def to_xml_cha(s):\n",
    "    s = re.sub('&','&#38;',s)\n",
    "    s = re.sub('>','&gt;',s)\n",
    "    s = re.sub('<','&lt;',s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Function for Unit code & Title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We extract the unitcode and title simultaneously as they locate in the same section.<br>\n",
    "**<font color=blue>Special case:</font>** there is some unit code **prefix with 4 digits**  which cannot be captured.<br>\n",
    "**<font color=blue>Regex adjustment:</font>**  `[A-Z]{3}\\d{4} ---> [A-Z]{3,4}\\d{4}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unit_code_title_extraction(unit_dict, unit_block, unit_str):\n",
    "    # unitcode & title extraction\n",
    "    pattern_unitc_title = re.compile(r'<span class=\"unitcode\">(?P<unitc>[A-Z]{3,4}\\d{4})</span> - (?P<title>.*)<span',re.M)\n",
    "    unitc = re.search(pattern_unitc_title,unit_block).group('unitc')\n",
    "    title = re.search(pattern_unitc_title,unit_block).group('title')\n",
    "    title = sub_xml_cha(title)\n",
    "    # add to the dict of each unit\n",
    "    unit_dict['@id'] = unitc\n",
    "    unit_dict['title'] = title\n",
    "    # for xml string\n",
    "    title_xml = to_xml_cha(title)\n",
    "    unit_str += f\"<unit id='{unitc}'>\\n\"\n",
    "    unit_str += f\"<title> {title_xml}</title>\\n\"\n",
    "    return (unit_dict, unit_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Function for synopsis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> As `<p>` and `</p>` tag define a paragraph in HTML. <br>\n",
    "We use Regex `>Synopsis</h2>\\n<div>\\n<p>` and `</p>` to locate synopsis between them.<br>\n",
    "**<font color=blue>Special case:</font>** units without synopsis <br>\n",
    "**<font color=blue>Logic adjustment:</font>** <br>\n",
    "    1. First use Regex re.search to match pattern \n",
    "    2. If return value ---> append to dict \n",
    "    3. IF no return ---> append NA    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# synopsis extraction\n",
    "def synopsis_extraction(unit_dict, unit_block, unit_str):\n",
    "    pattern_synopsis = re.compile(r'(?<=>Synopsis</h2>\\n<div>\\n<p>)(.*?)(?=</p>)',re.S)\n",
    "    synopsis_search = re.search(pattern_synopsis,unit_block)\n",
    "    if synopsis_search:\n",
    "        synopsis = synopsis_search.group()\n",
    "        synopsis = sub_xml_cha(synopsis)\n",
    "        pattern_link = re.compile(r'(?:(<.*?>))',re.S)\n",
    "        synopsis = re.sub(pattern_link,'',synopsis)\n",
    "        # for xml string\n",
    "        synopsis_xml = to_xml_cha(synopsis)\n",
    "        unit_str += f\"<synopsis> {synopsis_xml}</synopsis>\\n\"\n",
    "    else:\n",
    "        synopsis = 'NA'\n",
    "        unit_str += f\"<synopsis> NA </synopsis>\\n\"\n",
    "    unit_dict['synopsis'] = synopsis\n",
    "    return (unit_dict, unit_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Function for pre_requistics and prohibisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We handle these two at the same time as data is located in the same section: <br>\n",
    "1. Search keyword in the unit block, return nothing then append `NA` otherwise continue\n",
    "2. Match the paragraph which contains all three kinds of info\n",
    "3. In the matched paragraph, match the unitcode pattern \n",
    "4. If more than 1 unitcode returned, remove duplicated units <br>\n",
    ">The graph below provide the basic logic applied here.<br>\n",
    "\n",
    "> **<font color=blue>Special consideration:</font>** sample output only capture first `<p>...</p>` section <br>\n",
    "**<font color=blue>Potential improvement:</font>** fix Regex in step 2 to capture wider range of paragraph<br>    \n",
    "<img src = \"style/img_3_procedure.png\" height = \"300\" width = \"500\" style=\"float: left;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to capture unitcode for different keyword of pre- co-req and pro-\n",
    "def pre_pro_capture(keyword,unit_block):\n",
    "    if keyword in unit_block:\n",
    "        pattern_paragraph = re.compile(r'(?<={}</p>)(.*?)(?=</p>)'.format(keyword),re.S)\n",
    "        paragraph = re.search(pattern_paragraph,unit_block).group()\n",
    "        pattern_code = re.compile(r'[A-Z]{3,4}\\d{4}',re.S)\n",
    "        code_list = re.findall(pattern_code,paragraph)\n",
    "        code_list_uniq = list(set(code_list))\n",
    "    else:\n",
    "        code_list_uniq = []\n",
    "    return code_list_uniq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre_requistics & prohibisions extraction\n",
    "def pre_pro_extraction(unit_dict, unit_block, unit_str):\n",
    "    # combine prerequisites and co-requisites info \n",
    "    pre_list = pre_pro_capture('Prerequisites',unit_block)+ pre_pro_capture('Co-requisites',unit_block)\n",
    "    pre_list = list(set(pre_list)) # remove duplicate units\n",
    "    pro_list = pre_pro_capture('Prohibitions',unit_block) # extract prohibitions\n",
    "    \n",
    "    # combine prerequisites and co-requisites info \n",
    "    if not pre_list:\n",
    "        unit_dict['pre_requistics'] = 'NA'\n",
    "        unit_str += f\"<pre_requistics> NA </pre_requistics>\\n\"\n",
    "    elif len(pre_list)==1:\n",
    "        unit_dict['pre_requistics'] = {'pre_requistic':pre_list[0]}\n",
    "        unit_str += f\"<pre_requistics>\\n\"\\\n",
    "                    f\"<pre_requistic>{pre_list[0]}</pre_requistic>\"\\\n",
    "                    f\"</pre_requistics>\\n\"\n",
    "    else:\n",
    "        unit_dict['pre_requistics'] = {'pre_requistic':pre_list}\n",
    "        unit_str += f\"<pre_requistics>\\n\"\n",
    "        for each_pre in pre_list:\n",
    "            unit_str += f\"<pre_requistic>{each_pre}</pre_requistic>\"\n",
    "        unit_str += f\"</pre_requistics>\\n\"\n",
    "    \n",
    "    # apply the writing logic above to prohibition\n",
    "    if not pro_list:\n",
    "        unit_dict['prohibisions'] = 'NA'\n",
    "        unit_str += f\"<prohibisions> NA </prohibisions>\\n\"\n",
    "    elif len(pro_list)==1:\n",
    "        unit_dict['prohibisions'] = {'prohibision':pro_list[0]}\n",
    "        unit_str += f\"<prohibisions>\\n\"\\\n",
    "                    f\"<prohibision>{pro_list[0]}</prohibision>\"\\\n",
    "                    f\"</prohibisions>\\n\"\n",
    "    else:\n",
    "        unit_dict['prohibisions'] = {'prohibision':pro_list}\n",
    "        unit_str += f\"<prohibisions>\\n\"\n",
    "        for each_pro in pro_list:\n",
    "            unit_str += f\"<prohibision>{each_pro}</prohibision>\"\n",
    "        unit_str += f\"</prohibisions>\\n\"\n",
    "    return (unit_dict, unit_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Function for requirement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We still use `<p>` and `</p>` paragraph tag and keyword `Assessment` to locate the paragraph first. <br>\n",
    "> The rest procedures are similar. <br>\n",
    "> **<font color=blue>Special case:</font>** links are mixed with desired data `e.g. <...>` <br>\n",
    "> **<font color=blue>Regex adjustment:</font>** use `(?:(<.*?>))` to re.sub all links<br> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def req_extraction(unit_dict, unit_block, unit_str):\n",
    "    # Requirements extraction\n",
    "    pattern_req_paragraph = re.compile(r'(?<=>Assessment</h2>)(.*?)(?=</div>)',re.S)\n",
    "    req_paragraph_search = re.search(pattern_req_paragraph,unit_block)\n",
    "    if req_paragraph_search:\n",
    "        req_paragraph = req_paragraph_search.group()\n",
    "        pattern_req = re.compile(r'(?<=<p>)(.*?)(?=</p>)',re.S)\n",
    "        req_list = re.findall(pattern_req,req_paragraph)\n",
    "        pattern_link = re.compile(r'(?:(<.*?>))',re.S)\n",
    "        req_list = [re.sub(pattern_link,'',i) for i in req_list]\n",
    "        req_list = [sub_xml_cha(i) for i in req_list]\n",
    "        if not req_list:\n",
    "            unit_dict['requirements'] = 'NA'\n",
    "            unit_str += f\"<requirements> NA </requirements>\\n\"\n",
    "        elif len(req_list)==1:\n",
    "            unit_dict['requirements'] = {'requirement':req_list[0]}\n",
    "            # for xml string\n",
    "            req_xml = to_xml_cha(req_list[0])\n",
    "            unit_str += f\"<requirements>\\n\"\\\n",
    "                        f\"<requirement>{req_xml}</requirement>\"\\\n",
    "                        f\"</requirements>\\n\"\n",
    "        else:\n",
    "            unit_dict['requirements'] = {'requirement':req_list}\n",
    "            # for xml string\n",
    "            unit_str += f\"<requirements>\\n\"\n",
    "            for each_req in req_list:\n",
    "                req_xml = to_xml_cha(each_req)\n",
    "                unit_str += f\"<requirement>{req_xml}</requirement>\"\n",
    "            unit_str += f\"</requirements>\\n\"\n",
    "    else:\n",
    "        unit_dict['requirements'] = 'NA'\n",
    "        unit_str += f\"<requirements> NA </requirements>\\n\"\n",
    "    return (unit_dict, unit_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Function for outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Again, use `<p>` and `</p>` paragraph tag and keyword `Outcomes` to locate the paragraph first. <br>\n",
    "> Then extract the data and remove the `links <...>`. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outcome_extraction(unit_dict, unit_block, unit_str):\n",
    "    # Outcomes extraction\n",
    "    pattern_outc_paragraph = re.compile(r'(?<=>Outcomes</h2>)(.*?)(?=</div>)',re.S)\n",
    "    outc_paragraph_search = re.search(pattern_outc_paragraph,unit_block)\n",
    "    if outc_paragraph_search:\n",
    "        outc_paragraph = outc_paragraph_search.group()\n",
    "        pattern_outc = re.compile(r'(?<=<li>)(.*?)(?=</li>)',re.S)\n",
    "        pattern_link = re.compile(r'(?:(<.*?>))',re.S)\n",
    "        outc_list = re.findall(pattern_outc,outc_paragraph)\n",
    "        outc_list = [sub_xml_cha(i) for i in outc_list]\n",
    "        outc_list = [re.sub(pattern_link,'',i) for i in outc_list]\n",
    "        if not outc_list:\n",
    "            unit_dict['outcomes'] = 'NA'\n",
    "            unit_str += f\"<outcomes> NA </outcomes>\\n\"\n",
    "        elif len(outc_list)==1:\n",
    "            unit_dict['outcomes'] = {'outcome':outc_list[0]}\n",
    "            # for xml string\n",
    "            outc_xml = to_xml_cha(outc_list[0])\n",
    "            unit_str += f\"<outcomes>\\n\"\\\n",
    "                        f\"<outcome>{outc_xml}</outcome>\"\\\n",
    "                        f\"</outcomes>\\n\"\n",
    "        else:\n",
    "            unit_dict['outcomes'] = {'outcome':outc_list}\n",
    "            # for xml string\n",
    "            unit_str += f\"<outcomes>\\n\"\n",
    "            for each_outc in outc_list:\n",
    "                outc_xml = to_xml_cha(each_outc)\n",
    "                unit_str += f\"<outcome>{outc_xml}</outcome>\"\n",
    "            unit_str += f\"</outcomes>\\n\"\n",
    "    else:\n",
    "        #print(unit_dict['@id'],'outc')\n",
    "        unit_dict['outcomes'] = 'NA'\n",
    "        unit_str += f\"<outcomes> NA </outcomes>\\n\"\n",
    "    return (unit_dict, unit_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Function for Chief examiner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">1. use `<p>` and `</p>` paragraph tag and keyword `Chief examiner` to locate the paragraph first.<br>\n",
    ">2. Use Regex to extract the data\n",
    ">3. Append `TBA` insdead of `NA`<br>\n",
    "> **<font color=blue>Special consideration:</font>**<br> \n",
    "Assoc. Professor Bernard Flynn<br>\n",
    "Dr Rose-Marie Bezuidenhout<br>\n",
    "> **<font color=blue>Regex improvement:</font>**`[A-Za-z ]*` --->`[A-Z\\'a-z-\\. ]*`<br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ce_extraction(unit_dict, unit_block, unit_str):\n",
    "    # Chief_examiners extraction\n",
    "    pattern_ce_pragraph = re.compile(r'(?<=Chief examiner\\(s\\)</p>).*?(?=</p>)',re.S)\n",
    "    ce_pragraph_search = re.search(pattern_ce_pragraph,unit_block)\n",
    "    if ce_pragraph_search:\n",
    "        ce_pragraph = ce_pragraph_search.group()\n",
    "        pattern_ce = re.compile(r\"(?<=>)([A-Z\\'a-z-\\. ]*)(?=</a>)\",re.S)\n",
    "        ce_list = re.findall(pattern_ce,ce_pragraph)\n",
    "        if not ce_list:\n",
    "            unit_dict['chief_examiners'] = 'TBA'\n",
    "            unit_str += f\"<chief_examiners> TBA </chief_examiners>\\n\"\n",
    "        elif len(ce_list)==1:\n",
    "            unit_dict['chief_examiners'] = {'chief_examiner':ce_list[0]}\n",
    "            unit_str += f\"<chief_examiners>\\n\"\\\n",
    "                    f\"<chief_examiner>{ce_list[0]}</chief_examiner>\"\\\n",
    "                    f\"</chief_examiners>\\n\"\n",
    "        else:\n",
    "            unit_dict['chief_examiners'] = {'chief_examiner':ce_list}\n",
    "            unit_str += f\"<chief_examiners>\\n\"\n",
    "            for each_ce in ce_list:\n",
    "                unit_str += f\"<chief_examiner>{each_ce}</chief_examiner>\"\n",
    "            unit_str += f\"</chief_examiners>\\n\"\n",
    "    else:\n",
    "        unit_dict['chief_examiners'] = 'TBA'\n",
    "        unit_str += f\"<chief_examiners> TBA </chief_examiners>\\n\"\n",
    "    return (unit_dict, unit_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Combine ALL data into appropriate container ( dict for .json & str for .xml )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Finally, we can use different function to extract the data and construct the dict and str "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_list = []\n",
    "unit_str_all = '<?xml version=\"1.0\" encoding=\"UTF-8\" ?>\\n<units>\\n'\n",
    "\n",
    "for unit_block in unit_block_list:\n",
    "    unit_dict = {}\n",
    "    unit_str = ''\n",
    "    for keyword in test_output['units']['unit'][0].keys():\n",
    "        # construct an dict with keys and empty values\n",
    "        unit_dict[keyword] = None\n",
    "    # for each unit block, extract the info sequentially\n",
    "    # each function will return a dict and a string\n",
    "    unit_dict_1,unit_str_1 = unit_code_title_extraction(unit_dict, unit_block, unit_str)\n",
    "    unit_dict_2,unit_str_2 = synopsis_extraction(unit_dict_1, unit_block, unit_str_1)\n",
    "    unit_dict_3,unit_str_3 = pre_pro_extraction(unit_dict_2, unit_block, unit_str_2)\n",
    "    unit_dict_4,unit_str_4 = req_extraction(unit_dict_3, unit_block, unit_str_3)\n",
    "    unit_dict_5,unit_str_5 = outcome_extraction(unit_dict_4, unit_block, unit_str_4)\n",
    "    unit_dict_6,unit_str_6 = ce_extraction(unit_dict_5, unit_block, unit_str_5)\n",
    "    \n",
    "    unit_list.append(unit_dict_6)\n",
    "    unit_str_6 += \"</unit>\\n\"\n",
    "    unit_str_all += unit_str_6\n",
    "unit_str_all += \"</units>\"\n",
    "unit_dict_all = {'unit':unit_list}\n",
    "final_dict = {'units':unit_dict_all}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We can check the # of duplicate units in case of further processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unit code are 400\n",
      "The number of unique unit code are 379\n"
     ]
    }
   ],
   "source": [
    "unit_code_all = []\n",
    "for each in unit_list:\n",
    "    unit_code_all.append(each['@id'])\n",
    "print(f'The number of unit code are {len(unit_code_all)}')\n",
    "print(f'The number of unique unit code are {len(set(unit_code_all))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Write into json and xml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"json_file.json\",\"w\") as f:\n",
    "    json.dump(final_dict,f,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"xml_file.xml\",\"w\") as f:\n",
    "        f.write(unit_str_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> What we have done in this task:\n",
    "1. We create a **logic map** to handle the data extraction problems, break them into component parts.\n",
    "2. We use **html_tab-based approach** to identify common sections within a limited domain\n",
    "3. We use **Regex** as our text matching tool to identify small-scale structure in each section. \n",
    "3. Finally, we can retrieve data out of the semi-tructured data for further processing or text analytics. \n",
    "<img src = \"style/img_1_overview.png\" height = \"500\" width = \"500\" style=\"float: left;\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
