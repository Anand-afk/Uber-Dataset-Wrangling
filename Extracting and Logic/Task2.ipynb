{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "Date: 05.04.2019<br>\n",
    "Environment: Python 3.6.8 and Anaconda 4.6.7 (64-bit)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>body {\n",
       "    margin: 0;\n",
       "    font-family: Helvetica;\n",
       "}\n",
       "table.dataframe {\n",
       "    border-collapse: collapse;\n",
       "    border: none;\n",
       "}\n",
       "table.dataframe tr {\n",
       "    border: none;\n",
       "}\n",
       "table.dataframe td, table.dataframe th {\n",
       "    margin: 0;\n",
       "    border: 1px solid white;\n",
       "    padding-left: 0.25em;\n",
       "    padding-right: 0.25em;\n",
       "}\n",
       "table.dataframe th:not(:empty) {\n",
       "    background-color: #fec;\n",
       "    text-align: left;\n",
       "    font-weight: normal;\n",
       "}\n",
       "table.dataframe tr:nth-child(2) th:empty {\n",
       "    border-left: none;\n",
       "    border-right: 1px dashed #888;\n",
       "}\n",
       "table.dataframe td {\n",
       "    border: 2px solid #ccf;\n",
       "    background-color: #f4f4ff;\n",
       "}\n",
       "\n",
       "h2 {\n",
       "    color: white;\n",
       "    background-color: rgb(20, 167, 142);\n",
       "    padding: 0.5em;\n",
       "}\n",
       "h3 {\n",
       "    color: white;\n",
       "    background-color: rgb(20, 167, 142);\n",
       "    padding: 0.5em;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "css = open('style/style-table.css').read() + open('style/style-notebook.css').read()\n",
    "HTML('<style>{}</style>'.format(css))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    ">The main purpose of this report is to provide information about the methodology and process to solve this problem.<br>\n",
    "This report focuses on two parts:\n",
    "* Parsing pdf file to get pre-processed text. \n",
    "* The logics and implementation of exploring pre-processed text and generating features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Logic map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The procedures implemented in the text pre-processing task are as follows:\n",
    "<img src = \"style/img_4_logicmap.png\" height = \"500\" width = \"500\" style=\"float: left;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import ast\n",
    "import nltk \n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "from nltk.util import ngrams\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.probability import *\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import MWETokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Read xml data and excel data\n",
    "> Here, we use two methods to parse the pdf file:\n",
    ">1. First method is to use **`Adobe Acrobat`** to export the pdf into `.xml` file.\n",
    ">2. The second method is to use online tool **`Pdf to Excel`** to generate the `.csv`file.\n",
    ">3. Then we use the parsed text data generated from different tools to verify the correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-processing the xml file\n",
    "f = open('style/30086434.xml', 'r')\n",
    "data_str = f.read()  \n",
    "f.close()\n",
    "data_str = re.sub('&quot;','\"',data_str)\n",
    "data_str = re.sub('&amp;','&',data_str)\n",
    "\n",
    "# use </TR> as separator to split the file into 201 blocks and remove the first blank one\n",
    "unit_block_tr = re.split(r'</TR>',data_str)\n",
    "unit_doc_list_raw = unit_block_tr[1:]\n",
    "\n",
    "# pre-processing the string from xml file\n",
    "unit_doc_list = []\n",
    "for each_doc in unit_doc_list_raw[:-1]:\n",
    "    unit_doc_list.append(each_doc.lstrip('\\n\\n<TR>'))\n",
    "unit_doc_list.append(unit_doc_list_raw[-1].lstrip('\\n</Table>\\n\\n<'))\n",
    "\n",
    "# generate three lists for data extracted from xml file\n",
    "ucode_list = []\n",
    "out_list = []\n",
    "syn_list = []\n",
    "for i,each_doc in enumerate(unit_doc_list):\n",
    "    l = re.findall(r'(?<=>)(.*?)(?=<)',each_doc)\n",
    "    ucode_list.append(l[0].rstrip())\n",
    "    syn_list.append(l[1].rstrip())\n",
    "    out_list.append(l[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of unitcode: 200\n",
      "# of synopsis: 200\n",
      "# of outcomes: 200\n"
     ]
    }
   ],
   "source": [
    "# extract data from excel file\n",
    "excel_data = pd.ExcelFile('style/30086434-converted.xlsx')\n",
    "df = excel_data.parse('Table 1')\n",
    "df.head(2)\n",
    "# pre-processing the data based on the cross-check with data from xml file\n",
    "ucode_excel= [i for i in df['Title']]\n",
    "outc_excel = [i.replace('\\n',' ').lstrip('[').rstrip(']') for i in df['Outcomes']]\n",
    "df.loc[101,'Synopsis']='NA'\n",
    "syn_excel = [str(i).replace('\\n',' ') for i in df['Synopsis']]\n",
    "print(f'# of unitcode: {len(ucode_excel)}\\n'\\\n",
    "      f'# of synopsis: {len(syn_excel)}\\n'\\\n",
    "      f'# of outcomes: {len(outc_excel)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create the raw text dict\n",
    "> After we have the separate list, we can then build our initial raw text dict.<br>\n",
    "> **<font color=blue>Special consideration:</font>** \n",
    "    1. There are duplicate units in our dataset which means if we add them all sequentially, \n",
    "        the text for duplicate unit will be doubled.\n",
    "    2. Each outcome has [ ] around which probably means a list containing elements instead of one str. \n",
    "**<font color=blue>Adjustment solution:</font>** \n",
    "    1. We skip the existed key if it's already in the dict.\n",
    "    2. We still separate the synopsis and outcome as elements of a list at this stage.\n",
    "       The dict at this stage looks like: {unitcode : [[synopsis],[outcomes]], ...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duplicate unit: APG5666\n",
      "duplicate unit: ATS1339\n",
      "duplicate unit: ATS2354\n",
      "# of units in dict: 197\n"
     ]
    }
   ],
   "source": [
    "unit_dict = {}\n",
    "for i in range(200):\n",
    "    if ucode_excel[i] not in unit_dict.keys():\n",
    "        unit_dict[ucode_excel[i]]= []\n",
    "        unit_dict[ucode_excel[i]].append(syn_excel[i])\n",
    "        unit_dict[ucode_excel[i]].append(outc_excel[i])\n",
    "    else:\n",
    "        print(f'duplicate unit: {ucode_excel[i]}') # print duplicate units in case of further processing\n",
    "print(f'# of units in dict: {len(unit_dict)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of outcomes with []: 0\n"
     ]
    }
   ],
   "source": [
    "# double check if each outcome string is around with [ and ]\n",
    "count = 0\n",
    "for k,v in unit_dict.items():\n",
    "    if v[1].startswith('[') and v[1].endswith(']'):\n",
    "        count+=1\n",
    "print(f'# of outcomes with []: {count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Text Pre-processing\n",
    "### 6.1 Sentence Segmentation,  Tokenization and Case Normalization\n",
    "> In order to generate our initial vocab, we will implement the following sub-tasks: <br>\n",
    "    1. Sentence segmentation\n",
    "    2. Tokenization\n",
    "    3. Case Normalization\n",
    "> **<font color=blue>Special consideration:</font>** <br>\n",
    "> The bracket around outcomes need to be handled properly.<br>\n",
    "> * If we apply sent_tokenize directly on the outcome string, most of the time, it will return the whole outcome string instead of sentences in that brackets.<br>\n",
    "> * i.e. We expect that the return result of `['This is sentence A;','Another sentence B;']` should be `This is sentence A` and `Another sentence B` instead of a whole string `This is sentence A;','Another sentence B`.<br> \n",
    "> * As in the second senario, we will miss the case normalization for token **`Another`**. We might end up with **`Another`** and **`another`** appearing in the same text.<br>\n",
    "\n",
    "> **<font color=blue>Potential Solution:</font>** <br>\n",
    "> We can use **ast.literal_eval** function to transform the outcomes into meaningful sentence elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sen_Seg_Norm_Token(paragraph_str):\n",
    "    tokens_list = []\n",
    "    tokenizer = RegexpTokenizer(r\"\\w+(?:[-']\\w+)?\", gaps=False)\n",
    "    # Sentence segmentation for the paragraph\n",
    "    for sen in sent_tokenize(paragraph_str):\n",
    "        sen_tokens = tokenizer.tokenize(sen)\n",
    "        # for each token in the sentence, normalize to lower case.\n",
    "        if sen_tokens:\n",
    "            sen_tokens[0] = sen_tokens[0].lower()\n",
    "        tokens_list.extend(sen_tokens)\n",
    "    return tokens_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_tokens_raw = {}\n",
    "for u_code,u_info_list in unit_dict.items():\n",
    "    # combine the Synopsis and Outcomes\n",
    "    unit_info=u_info_list[0]+' '+u_info_list[1]\n",
    "    # pass whole str into tokenization function\n",
    "    raw_tokens = sen_Seg_Norm_Token(unit_info)\n",
    "    unit_tokens_raw[u_code]=raw_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Overview of our initial vocab bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size : 4442\n",
      "Total # of tokens: 31275\n",
      "Lexical diversity: 7.040747411076092\n"
     ]
    }
   ],
   "source": [
    "corpus_tokens = list(chain.from_iterable(unit_tokens_raw.values()))\n",
    "print (f'Vocabulary size : {len(set(corpus_tokens))}'\\\n",
    "       f'\\nTotal # of tokens: {len(corpus_tokens)}'\\\n",
    "       f'\\nLexical diversity: {len(corpus_tokens)/len(set(corpus_tokens))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Generate the 200 bigram collocations\n",
    "> * Now we use our tokens generated to generate the 1st 200 meaningful bigrams collocations.<br>\n",
    "> * We've already used the chain.frome_iterable function to concatenate all the tokens. <br>\n",
    "    The returned list  `corpus_tokens ` contains a list of all the words seprated by while space.<br>\n",
    "> * **PMI scores** are used to find the best 200 bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## using pmi measure to find 200 meaning ful bigrams\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "bigram_finder = nltk.collocations.BigramCollocationFinder.from_words(corpus_tokens)\n",
    "bigram_200 = bigram_finder.nbest(bigram_measures.pmi, 200)\n",
    "bigram_200_list = [f'{a}_{b}' for a,b in bigram_200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Re-tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Now it's time to replace the corresponding unigrams by the bigram collocations generated. \n",
    "> * We can use the **MWETokenizer** to re-tokenize the sentence with multi-word expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mwe_tokenizer = MWETokenizer(bigram_200)\n",
    "unit_tokens_mwe = {u_code:mwe_tokenizer.tokenize(tokens) for u_code,tokens in unit_tokens_raw.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size after retokenization : 4279\n",
      "Total # of tokens after retokenization: 31112\n",
      "Lexical diversity: 7.270857677027343\n"
     ]
    }
   ],
   "source": [
    "corpus_tokens_mwe = list(chain.from_iterable(unit_tokens_mwe.values()))\n",
    "print (f'Vocabulary size after retokenization : {len(set(corpus_tokens_mwe))}'\\\n",
    "       f'\\nTotal # of tokens after retokenization: {len(corpus_tokens_mwe)}'\\\n",
    "       f'\\nLexical diversity: {len(corpus_tokens_mwe)/len(set(corpus_tokens_mwe))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Removing 'Bad Features' ( i.e. stop words, the most and less frequent words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **<font color=blue>Stop Words Removal</font>**\n",
    "> * Define a **filter function** for easily filter our token list. \n",
    "> * Define and remove the context-independent function words from each token list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_Filter(token_list, undesired_list):\n",
    "    return [w for w in token_list if w not in undesired_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = []\n",
    "with open('style/stopwords_en.txt') as f:\n",
    "    stopwords = f.read().splitlines()\n",
    "# get rid of the stopwords and construct the new dict \n",
    "unit_tokens_Sw = {u_code:token_Filter(tokens, stopwords) for u_code,tokens in unit_tokens_mwe.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size after removing stopwords : 4045\n",
      "Total # of tokens after after removing stopwords: 18673\n",
      "Lexical diversity: 4.616316440049443\n"
     ]
    }
   ],
   "source": [
    "corpus_tokens_Sw = list(chain.from_iterable(unit_tokens_Sw.values()))\n",
    "print (f'Vocab size after removing stopwords : {len(set(corpus_tokens_Sw))}'\\\n",
    "       f'\\nTotal # of tokens after after removing stopwords: {len(corpus_tokens_Sw)}'\\\n",
    "       f'\\nLexical diversity: {len(corpus_tokens_Sw)/len(set(corpus_tokens_Sw))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **<font color=blue>Frequent and Rare words Removal based on Document Frequency</font>**\n",
    "> * We put all the tokens in a list using **chain.from_iterable** and past it to **FreqDist**.\n",
    "> * The set makes sure that each word in an article appears only once, thus the total number of \n",
    "    times a word appears in all the sets is equal to the number of documents containing that word.\n",
    "> * Then we filter out the bad context-dependent according to threshould"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(chain.from_iterable([set(token) for token in unit_tokens_Sw.values()]))\n",
    "doc_freq = FreqDist(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('unit', 155),\n",
       " ('students', 107),\n",
       " ('skills', 84),\n",
       " ('understanding', 75),\n",
       " ('analyse', 73),\n",
       " ('apply', 72),\n",
       " ('knowledge', 64),\n",
       " ('develop', 62),\n",
       " ('critically', 60),\n",
       " ('evaluate', 60),\n",
       " ('research', 58),\n",
       " ('issues', 56),\n",
       " ('development', 55),\n",
       " ('including', 55),\n",
       " ('practice', 54),\n",
       " ('identify', 53),\n",
       " ('analysis', 46),\n",
       " ('demonstrate', 46),\n",
       " ('work', 44),\n",
       " ('principles', 43),\n",
       " ('range', 42),\n",
       " ('key', 42),\n",
       " ('design', 41),\n",
       " ('techniques', 41),\n",
       " ('topics', 41),\n",
       " ('concepts', 41),\n",
       " ('methods', 40),\n",
       " ('professional', 40),\n",
       " ('management', 39),\n",
       " ('problems', 38),\n",
       " ('contemporary', 37),\n",
       " ('critical', 37),\n",
       " ('describe', 36),\n",
       " ('social', 36),\n",
       " ('ability', 36),\n",
       " ('health', 35),\n",
       " ('role', 34),\n",
       " ('strategies', 34),\n",
       " ('communication', 34),\n",
       " ('project', 33),\n",
       " ('systems', 33),\n",
       " ('relevant', 33),\n",
       " ('discuss', 33),\n",
       " ('theories', 32),\n",
       " ('theoretical', 32),\n",
       " ('explain', 32),\n",
       " ('understand', 32),\n",
       " ('environment', 32),\n",
       " ('major', 31),\n",
       " ('effectively', 31),\n",
       " ('studies', 31),\n",
       " ('data', 31),\n",
       " ('information', 31),\n",
       " ('context', 30),\n",
       " ('study', 30),\n",
       " ('written', 30),\n",
       " ('processes', 29),\n",
       " ('examine', 29),\n",
       " ('communicate', 29),\n",
       " ('cultural', 29),\n",
       " ('field', 28),\n",
       " ('tools', 27),\n",
       " ('theory', 27),\n",
       " ('related', 27),\n",
       " ('practical', 27),\n",
       " ('advanced', 26),\n",
       " ('application', 26),\n",
       " ('approaches', 26),\n",
       " ('writing', 26),\n",
       " ('assess', 25),\n",
       " ('focus', 25),\n",
       " ('provide', 25),\n",
       " ('process', 24),\n",
       " ('ethical', 24),\n",
       " ('basic', 24),\n",
       " ('models', 24),\n",
       " ('include', 24),\n",
       " ('learning', 24),\n",
       " ('examines', 24),\n",
       " ('challenges', 24),\n",
       " ('practices', 24),\n",
       " ('current', 23),\n",
       " ('fundamental', 22),\n",
       " ('level', 22),\n",
       " ('assessment', 22),\n",
       " ('complex', 22),\n",
       " ('introduces', 21),\n",
       " ('impact', 21),\n",
       " ('historical', 21),\n",
       " ('evidence', 21),\n",
       " ('culture', 21),\n",
       " ('developed', 20),\n",
       " ('Understand', 20),\n",
       " ('explore', 20),\n",
       " ('contexts', 20),\n",
       " ('literature', 20),\n",
       " ('oral', 20),\n",
       " ('present', 20),\n",
       " ('clinical', 20),\n",
       " ('ideas', 19),\n",
       " ('safety', 19),\n",
       " ('types', 19),\n",
       " ('behaviour', 19),\n",
       " ('presentation', 19),\n",
       " ('reflect', 19),\n",
       " ('activities', 19),\n",
       " ('international', 19),\n",
       " ('based', 18),\n",
       " ('environments', 18),\n",
       " ('physical', 18),\n",
       " ('individual', 18),\n",
       " ('history', 18),\n",
       " ('undertake', 18),\n",
       " ('Demonstrate', 18),\n",
       " ('language', 18),\n",
       " ('case', 18),\n",
       " ('aspects', 18),\n",
       " ('developing', 17),\n",
       " ('experience', 17),\n",
       " ('occupational', 17),\n",
       " ('interpret', 17),\n",
       " ('awareness', 17),\n",
       " ('conduct', 17),\n",
       " ('problem', 17),\n",
       " ('system', 17),\n",
       " ('variety', 17),\n",
       " ('care', 17),\n",
       " ('specific', 17),\n",
       " ('material', 16),\n",
       " ('production', 16),\n",
       " ('findings', 16),\n",
       " ('diverse', 16),\n",
       " ('community', 16),\n",
       " ('change', 16),\n",
       " ('form', 16),\n",
       " ('relationship', 16),\n",
       " ('business', 16),\n",
       " ('report', 16),\n",
       " ('scientific', 16),\n",
       " ('political', 16),\n",
       " ('world', 16),\n",
       " ('modern', 16),\n",
       " ('effective', 16),\n",
       " ('emphasis', 16),\n",
       " ('legal', 16),\n",
       " ('structure', 16),\n",
       " ('nature', 16),\n",
       " ('media', 15),\n",
       " ('rules', 15),\n",
       " ('plan', 15),\n",
       " ('organisation', 15),\n",
       " ('human', 15),\n",
       " ('critique', 15),\n",
       " ('area', 15),\n",
       " ('approach', 15),\n",
       " ('team', 15),\n",
       " ('applied', 14),\n",
       " ('discipline', 14),\n",
       " ('requirements', 14),\n",
       " ('terms', 14),\n",
       " ('introduction', 14),\n",
       " ('technology', 14),\n",
       " ('manage', 14),\n",
       " ('conditions', 14),\n",
       " ('applications', 14),\n",
       " ('personal', 14),\n",
       " ('texts', 14),\n",
       " ('compare', 14),\n",
       " ('factors', 14),\n",
       " ('software', 13),\n",
       " ('important', 13),\n",
       " ('society', 13),\n",
       " ('Apply', 13),\n",
       " ('structures', 13),\n",
       " ('designed', 13),\n",
       " ('appraise', 13),\n",
       " ('covered', 13),\n",
       " ('technologies', 13),\n",
       " ('performance', 13),\n",
       " ('sources', 13),\n",
       " ('group', 13),\n",
       " ('explores', 13),\n",
       " ('relationships', 13),\n",
       " ('recognise', 13),\n",
       " ('core', 13),\n",
       " ('review', 13),\n",
       " ('technical', 13),\n",
       " ('required', 13),\n",
       " ('laboratory', 13),\n",
       " ('focuses', 13),\n",
       " ('visual', 12),\n",
       " ('elements', 12),\n",
       " ('disciplines', 12),\n",
       " ('setting', 12),\n",
       " ('na', 12),\n",
       " ('materials', 12),\n",
       " ('results', 12),\n",
       " ('life', 12),\n",
       " ('organisational', 12),\n",
       " ('order', 12),\n",
       " ('academic', 12),\n",
       " ('involved', 12),\n",
       " ('body', 12),\n",
       " ('solving', 12),\n",
       " ('ways', 12),\n",
       " ('Identify', 12),\n",
       " ('relation', 12),\n",
       " ('outcomes', 12),\n",
       " ('quality', 12),\n",
       " ('synthesise', 12),\n",
       " ('model', 12),\n",
       " ('introduce', 12),\n",
       " ('aims', 12),\n",
       " ('examined', 11),\n",
       " ('arguments', 11),\n",
       " ('modelling', 11),\n",
       " ('opportunity', 11),\n",
       " ('frameworks', 11),\n",
       " ('environmental', 11),\n",
       " ('energy', 11),\n",
       " ('function', 11),\n",
       " ('formulate', 11),\n",
       " ('settings', 11),\n",
       " ('student', 11),\n",
       " ('limitations', 11),\n",
       " ('engage', 11),\n",
       " ('Australian', 11),\n",
       " ('examples', 11),\n",
       " ('economic', 11),\n",
       " ('affect', 11),\n",
       " ('prepare', 11),\n",
       " ('innovative', 11),\n",
       " ('projects', 11),\n",
       " ('working', 11),\n",
       " ('influence', 11),\n",
       " ('reading', 11),\n",
       " ('planning', 11),\n",
       " ('effects', 10),\n",
       " ('time', 10),\n",
       " ('1', 10),\n",
       " ('global', 10),\n",
       " ('capacity', 10),\n",
       " ('main', 10),\n",
       " ('analysing', 10),\n",
       " ('politics', 10),\n",
       " ('future', 10),\n",
       " ('select', 10),\n",
       " ('organisations', 10),\n",
       " ('framework', 10),\n",
       " ('areas', 10),\n",
       " ('power', 10),\n",
       " ('basis', 10),\n",
       " ('experimental', 10),\n",
       " ('simple', 10),\n",
       " ('common', 10),\n",
       " ('conceptual', 10),\n",
       " ('solutions', 10),\n",
       " ('questions', 10),\n",
       " ('industry', 10),\n",
       " ('presentations', 10),\n",
       " ('debates', 10),\n",
       " ('lectures', 10),\n",
       " ('enhance', 10),\n",
       " ('comprehensive', 10),\n",
       " ('national', 10),\n",
       " ('Australia', 10),\n",
       " ('programs', 10),\n",
       " ('forms', 10),\n",
       " ('mechanisms', 10),\n",
       " ('policies', 10),\n",
       " ('importance', 10),\n",
       " ('studied', 10),\n",
       " ('competence', 10),\n",
       " ('control', 10),\n",
       " ('policy', 10),\n",
       " ('implement', 9),\n",
       " ('potential', 9),\n",
       " ('creative', 9),\n",
       " ('investigate', 9),\n",
       " ('Critically', 9),\n",
       " ('making', 9),\n",
       " ('general', 9),\n",
       " ('changing', 9),\n",
       " ('perspectives', 9),\n",
       " ('broad', 9),\n",
       " ('properties', 9),\n",
       " ('nursing', 9),\n",
       " ('services', 9),\n",
       " ('evidence-based', 9),\n",
       " ('selected', 9),\n",
       " ('resources', 9),\n",
       " ('public', 9),\n",
       " ('features', 9),\n",
       " ('practitioners', 9),\n",
       " ('industries', 9),\n",
       " ('integrated', 9),\n",
       " ('finance', 9),\n",
       " ('thinking', 9),\n",
       " ('learn', 9),\n",
       " ('introduced', 9),\n",
       " ('situations', 9),\n",
       " ('interventions', 9),\n",
       " ('standards', 9),\n",
       " ('analytical', 9),\n",
       " ('component', 9),\n",
       " ('chosen', 9),\n",
       " ('broader', 9),\n",
       " ('acquired', 9),\n",
       " ('create', 8),\n",
       " ('integrate', 8),\n",
       " ('South', 8),\n",
       " ('science', 8),\n",
       " ('functions', 8),\n",
       " ('objects', 8),\n",
       " ('architecture', 8),\n",
       " ('utilising', 8),\n",
       " ('languages', 8),\n",
       " ('generate', 8),\n",
       " ('dynamic', 8),\n",
       " ('relating', 8),\n",
       " ('emerging', 8),\n",
       " ('identity', 8),\n",
       " ('acquisition', 8),\n",
       " ('relevance', 8),\n",
       " ('chemical', 8),\n",
       " ('strategy', 8),\n",
       " ('program', 8),\n",
       " ('works', 8),\n",
       " ('risk', 8),\n",
       " ('cultures', 8),\n",
       " ('primary', 8),\n",
       " ('units', 8),\n",
       " ('part', 8),\n",
       " ('local', 8),\n",
       " ('responsibilities', 8),\n",
       " ('disease', 8),\n",
       " ('law', 8),\n",
       " ('workplace', 8),\n",
       " ('considered', 8),\n",
       " ('components', 8),\n",
       " ('means', 8),\n",
       " ('engineering', 8),\n",
       " ('gain', 8),\n",
       " ('values', 8),\n",
       " ('collection', 8),\n",
       " ('engagement', 8),\n",
       " ('characteristics', 8),\n",
       " ('reference', 8),\n",
       " ('implications', 8),\n",
       " ('applying', 8),\n",
       " ('evaluation', 8),\n",
       " ('computing', 7),\n",
       " ('sound', 7),\n",
       " ('action', 7),\n",
       " ('discussion', 7),\n",
       " ('perspective', 7),\n",
       " ('question', 7),\n",
       " ('decision-making', 7),\n",
       " ('philosophy', 7),\n",
       " ('solve', 7),\n",
       " ('mathematical', 7),\n",
       " ('resolution', 7),\n",
       " ('peers', 7),\n",
       " ('Design', 7),\n",
       " ('respond', 7),\n",
       " ('decision', 7),\n",
       " ('individuals', 7),\n",
       " ('solid', 7),\n",
       " ('address', 7),\n",
       " ('topic', 7),\n",
       " ('forces', 7),\n",
       " ('difference', 7),\n",
       " ('transfer', 7),\n",
       " ('informed', 7),\n",
       " ('themes', 7),\n",
       " ('skill', 7),\n",
       " ('aim', 7),\n",
       " ('patients', 7),\n",
       " ('equations', 7),\n",
       " ('Analyse', 7),\n",
       " ('reports', 7),\n",
       " ('methodologies', 7),\n",
       " ('independent', 7),\n",
       " ('investigation', 7),\n",
       " ('completion', 7),\n",
       " ('utilise', 7),\n",
       " ('employ', 7),\n",
       " ('responses', 7),\n",
       " ('locate', 7),\n",
       " ('suitable', 7),\n",
       " ('countries', 7),\n",
       " ('roles', 7),\n",
       " ('explored', 7),\n",
       " ('online', 7),\n",
       " ('groups', 7),\n",
       " ('experiences', 7),\n",
       " ('includes', 7),\n",
       " ('wide', 7),\n",
       " ('therapeutic', 7),\n",
       " ('scope', 7),\n",
       " ('assessments', 7),\n",
       " ('interaction', 7),\n",
       " ('evolution', 7),\n",
       " ('identifying', 7),\n",
       " ('marketing', 7),\n",
       " ('covers', 7),\n",
       " ('decisions', 7),\n",
       " ('tasks', 7),\n",
       " ('undertaken', 7),\n",
       " ('safe', 7),\n",
       " ('incorporate', 6),\n",
       " ('position', 6),\n",
       " ('relations', 6),\n",
       " ('central', 6),\n",
       " ('integration', 6),\n",
       " ('solution', 6),\n",
       " ('creation', 6),\n",
       " ('art', 6),\n",
       " ('build', 6),\n",
       " ('addition', 6),\n",
       " ('proposals', 6),\n",
       " ('effect', 6),\n",
       " ('strategic', 6),\n",
       " ('opportunities', 6),\n",
       " ('managing', 6),\n",
       " ('economics', 6),\n",
       " ('designs', 6),\n",
       " ('method', 6),\n",
       " ('analyses', 6),\n",
       " ('managers', 6),\n",
       " ('empirical', 6),\n",
       " ('operation', 6),\n",
       " ('medical', 6),\n",
       " ('fluid', 6),\n",
       " ('underlying', 6),\n",
       " ('biological', 6),\n",
       " ('relate', 6),\n",
       " ('century', 6),\n",
       " ('collaborative', 6),\n",
       " ('market', 6),\n",
       " ('assist', 6),\n",
       " ('demonstrated', 6),\n",
       " ('Assess', 6),\n",
       " ('Evaluate', 6),\n",
       " ('criteria', 6),\n",
       " ('treatment', 6),\n",
       " ('develops', 6),\n",
       " ('document', 6),\n",
       " ('feedback', 6),\n",
       " ('formal', 6),\n",
       " ('construct', 6),\n",
       " ('discussions', 6),\n",
       " ('interest', 6),\n",
       " ('incorporating', 6),\n",
       " ('task', 6),\n",
       " ('portfolio', 6),\n",
       " ('communities', 6),\n",
       " ('service', 6),\n",
       " ('conflicts', 6),\n",
       " ('presented', 6),\n",
       " ('government', 6),\n",
       " ('audience', 6),\n",
       " ('comparative', 6),\n",
       " ('purposes', 6),\n",
       " ('Be', 6),\n",
       " ('patient', 6),\n",
       " ('genetic', 6),\n",
       " ('ethics', 6),\n",
       " ('interpreting', 6),\n",
       " ('internationally', 6),\n",
       " ('commonly', 6),\n",
       " ('complexity', 6),\n",
       " ('support', 6),\n",
       " ('parameters', 6),\n",
       " ('dynamics', 6),\n",
       " ('accurately', 6),\n",
       " ('participation', 6),\n",
       " ('delivery', 6),\n",
       " ('significant', 6),\n",
       " ('institutions', 6),\n",
       " ('requirement', 6),\n",
       " ('read', 6),\n",
       " ('expected', 6),\n",
       " ('logical', 6),\n",
       " ('contribution', 6),\n",
       " ('contrast', 6),\n",
       " ('trends', 6),\n",
       " ('achieve', 6),\n",
       " ('consumers', 6),\n",
       " ('developments', 6),\n",
       " ('discourses', 6),\n",
       " ('security', 6),\n",
       " ('provided', 6),\n",
       " ('gained', 6),\n",
       " ('early', 6),\n",
       " ('Explain', 6),\n",
       " ('methodological', 6),\n",
       " ('equipment', 6),\n",
       " ('subject', 6),\n",
       " ('Describe', 6),\n",
       " ('procedures', 6),\n",
       " ('healthcare', 6),\n",
       " ('appropriately', 6),\n",
       " ('methodology', 6),\n",
       " ('greater', 6),\n",
       " ('class', 6),\n",
       " ('teamwork', 6),\n",
       " ('food', 6),\n",
       " ('formation', 6),\n",
       " ('verbal', 6),\n",
       " ('final', 6),\n",
       " ('Discuss', 6),\n",
       " ('make', 6),\n",
       " ('transport', 6),\n",
       " ('set', 6),\n",
       " ('stakeholders', 6),\n",
       " ('place', 6),\n",
       " ('dimensions', 6),\n",
       " ('Practice', 6),\n",
       " ('builds', 5),\n",
       " ('original', 5),\n",
       " ('3D', 5),\n",
       " ('digital', 5),\n",
       " ('product', 5),\n",
       " ('studio', 5),\n",
       " ('accordance', 5),\n",
       " ('Africa', 5),\n",
       " ('view', 5),\n",
       " ('construction', 5),\n",
       " ('perform', 5),\n",
       " ('variables', 5),\n",
       " ('imaging', 5),\n",
       " ('intensive', 5),\n",
       " ('collaboratively', 5),\n",
       " ('implementing', 5),\n",
       " ('exploration', 5),\n",
       " ('exchange', 5),\n",
       " ('year', 5),\n",
       " ('translation', 5),\n",
       " ('large', 5),\n",
       " ('obtain', 5),\n",
       " ('structural', 5),\n",
       " ('state', 5),\n",
       " ('face', 5),\n",
       " ('member', 5),\n",
       " ('conducting', 5),\n",
       " ('cells', 5),\n",
       " ('play', 5),\n",
       " ('light', 5),\n",
       " ('therapy', 5),\n",
       " ('movement', 5),\n",
       " ('ethnicity', 5),\n",
       " ('appreciation', 5),\n",
       " ('period', 5),\n",
       " ('distinguish', 5),\n",
       " ('intervention', 5),\n",
       " ('developmental', 5),\n",
       " ('modules', 5),\n",
       " ('successful', 5),\n",
       " ('leadership', 5),\n",
       " ('identification', 5),\n",
       " ('series', 5),\n",
       " ('Develop', 5),\n",
       " ('intellectual', 5),\n",
       " ('creating', 5),\n",
       " ('cases', 5),\n",
       " ('levels', 5),\n",
       " ('resource', 5),\n",
       " ('past', 5),\n",
       " ('site', 5),\n",
       " ('complete', 5),\n",
       " ('format', 5),\n",
       " ('summative', 5),\n",
       " ('families', 5),\n",
       " ('experienced', 5),\n",
       " ('built', 5),\n",
       " ('drugs', 5),\n",
       " ('response', 5),\n",
       " ('presents', 5),\n",
       " ('justice', 5),\n",
       " ('teaching', 5),\n",
       " ('impacts', 5),\n",
       " ('patterns', 5),\n",
       " ('building', 5),\n",
       " ('test', 5),\n",
       " ('technological', 5),\n",
       " ('multi-disciplinary', 5),\n",
       " ('children', 5),\n",
       " ('options', 5),\n",
       " ('orally', 5),\n",
       " ('people', 5),\n",
       " ('psychological', 5),\n",
       " ('populations', 5),\n",
       " ('clients', 5),\n",
       " ('advocacy', 5),\n",
       " ('mental', 5),\n",
       " ('manner', 5),\n",
       " ('constraints', 5),\n",
       " ('representation', 5),\n",
       " ('classes', 5),\n",
       " ('discussed', 5),\n",
       " ('meet', 5),\n",
       " ('physiology', 5),\n",
       " ('gathering', 5),\n",
       " ('growth', 5),\n",
       " ('articulate', 5),\n",
       " ('events', 5),\n",
       " ('made', 5),\n",
       " ('express', 5),\n",
       " ('choice', 5),\n",
       " ('processing', 5),\n",
       " ('2', 5),\n",
       " ('emergency', 5),\n",
       " ('extended', 5),\n",
       " ('practitioner', 5),\n",
       " ('quantitative', 5),\n",
       " ('reporting', 5),\n",
       " ('qualitative', 5),\n",
       " ('marine', 5),\n",
       " ('maintain', 5),\n",
       " ('differences', 5),\n",
       " ('gender', 5),\n",
       " ('air', 5),\n",
       " ('exercises', 5),\n",
       " ('fields', 5),\n",
       " ('real', 5),\n",
       " ('normal', 5),\n",
       " ('nutrition', 5),\n",
       " ('motion', 4),\n",
       " ('live', 4),\n",
       " ('enhanced', 4),\n",
       " ('content', 4),\n",
       " ('manipulation', 4),\n",
       " ('multimedia', 4),\n",
       " ('family', 4),\n",
       " ('interpretation', 4),\n",
       " ('argument', 4),\n",
       " ('African', 4),\n",
       " ('point', 4),\n",
       " ('points', 4),\n",
       " ('partial', 4),\n",
       " ('computer', 4),\n",
       " ('interactions', 4),\n",
       " ('modes', 4),\n",
       " ('An', 4),\n",
       " ('A', 4),\n",
       " ('plans', 4),\n",
       " ('interviews', 4),\n",
       " ('search', 4),\n",
       " ('The', 4),\n",
       " ('University', 4),\n",
       " ('stress', 4),\n",
       " ('devise', 4),\n",
       " ('responsibility', 4),\n",
       " ('employees', 4),\n",
       " ('staff', 4),\n",
       " ('secondary', 4),\n",
       " ('laws', 4),\n",
       " ('metabolism', 4),\n",
       " ('flow', 4),\n",
       " ('physics', 4),\n",
       " ('radiation', 4),\n",
       " ('race', 4),\n",
       " ('twentieth', 4),\n",
       " ('background', 4),\n",
       " ('revolution', 4),\n",
       " ('actors', 4),\n",
       " ('war', 4),\n",
       " ('World', 4),\n",
       " ('standard', 4),\n",
       " ('implementation', 4),\n",
       " ('produced', 4),\n",
       " ('goals', 4),\n",
       " ('delivered', 4),\n",
       " ('module', 4),\n",
       " ('considers', 4),\n",
       " ('propose', 4),\n",
       " ('efficacy', 4),\n",
       " ('numerical', 4),\n",
       " ('Formulate', 4),\n",
       " ('interests', 4),\n",
       " ('terminology', 4),\n",
       " ('sophisticated', 4),\n",
       " ('Recognise', 4),\n",
       " ('encouraged', 4),\n",
       " ('composition', 4),\n",
       " ('Use', 4),\n",
       " ('meaning', 4),\n",
       " ('statistical', 4),\n",
       " ('employed', 4),\n",
       " ('studying', 4),\n",
       " ('presenting', 4),\n",
       " ('participate', 4),\n",
       " ('networks', 4),\n",
       " ('German', 4),\n",
       " ('criticism', 4),\n",
       " ('competencies', 4),\n",
       " ('markets', 4),\n",
       " ('programming', 4),\n",
       " ('dealing', 4),\n",
       " ('financial', 4),\n",
       " ('women', 4),\n",
       " ('significance', 4),\n",
       " ('concerns', 4),\n",
       " ('relates', 4),\n",
       " ('matters', 4),\n",
       " ('expert', 4),\n",
       " ('selection', 4),\n",
       " ('governing', 4),\n",
       " ('underpinnings', 4),\n",
       " ('enterprise', 4),\n",
       " ('concept', 4),\n",
       " ('capabilities', 4),\n",
       " ('challenge', 4),\n",
       " ('improving', 4),\n",
       " ('reviewed', 4),\n",
       " ('placement', 4),\n",
       " ('professionally', 4),\n",
       " ('education', 4),\n",
       " ('corporate', 4),\n",
       " ('East', 4),\n",
       " ('supporting', 4),\n",
       " ('cycle', 4),\n",
       " ('client', 4),\n",
       " ('recommendations', 4),\n",
       " ('result', 4),\n",
       " ('movements', 4),\n",
       " ('rigorous', 4),\n",
       " ('proposal', 4),\n",
       " ('chronic', 4),\n",
       " ('narratives', 4),\n",
       " ('small', 4),\n",
       " ('semester', 4),\n",
       " ('audiences', 4),\n",
       " ('supervisor', 4),\n",
       " ('mechanical', 4),\n",
       " ('involving', 4),\n",
       " ('contract', 4),\n",
       " ('acknowledging', 4),\n",
       " ('highlighted', 4),\n",
       " ('behavioural', 4),\n",
       " ('supported', 4),\n",
       " ('pathways', 4),\n",
       " ('molecules', 4),\n",
       " ('target', 4),\n",
       " ('acquire', 4),\n",
       " ('internal', 4),\n",
       " ('external', 4),\n",
       " ('shaping', 4),\n",
       " ('drawing', 4),\n",
       " ('provision', 4),\n",
       " ('English', 4),\n",
       " ('good', 4),\n",
       " ('readings', 4),\n",
       " ('unique', 4),\n",
       " ('prevention', 4),\n",
       " ('spoken', 4),\n",
       " ('conventional', 4),\n",
       " ('clear', 4),\n",
       " ('goal', 4),\n",
       " ('distribution', 4),\n",
       " ('encountered', 4),\n",
       " ('addresses', 4),\n",
       " ('acute', 4),\n",
       " ('regimes', 4),\n",
       " ('population', 4),\n",
       " ('listening', 4),\n",
       " ('expression', 4),\n",
       " ('experiencing', 4),\n",
       " ('encompass', 4),\n",
       " ('diversity', 4),\n",
       " ('designing', 4),\n",
       " ('supervised', 4),\n",
       " ('essential', 4),\n",
       " ('availability', 4),\n",
       " ('foundation', 4),\n",
       " ('deeper', 4),\n",
       " ('Have', 4),\n",
       " ('gas', 4),\n",
       " ('measure', 4),\n",
       " ('pathophysiology', 4),\n",
       " ('lead', 4),\n",
       " ('degree', 4),\n",
       " ('synthesis', 4),\n",
       " ('protein', 4),\n",
       " ('governance', 4),\n",
       " ('endeavour', 4),\n",
       " ('involves', 4),\n",
       " ('define', 4),\n",
       " ('overview', 4),\n",
       " ('industrial', 4),\n",
       " ('short', 4),\n",
       " ('high', 4),\n",
       " ('underpin', 4),\n",
       " ('access', 4),\n",
       " ('failure', 4),\n",
       " ('aware', 4),\n",
       " ('effectiveness', 4),\n",
       " ('link', 4),\n",
       " ('enable', 4),\n",
       " ('outline', 4),\n",
       " ('notions', 4),\n",
       " ('coordinator', 4),\n",
       " ('measures', 4),\n",
       " ('cover', 4),\n",
       " ('speaking', 4),\n",
       " ('consultation', 4),\n",
       " ('providing', 4),\n",
       " ('mass', 4),\n",
       " ('learnt', 3),\n",
       " ('video', 3),\n",
       " ('multiple', 3),\n",
       " ('examination', 3),\n",
       " ('conflict', 3),\n",
       " ('textual', 3),\n",
       " ('moral', 3),\n",
       " ('arise', 3),\n",
       " ('consequences', 3),\n",
       " ('reasons', 3),\n",
       " ('linear', 3),\n",
       " ('optimisation', 3),\n",
       " ('resulting', 3),\n",
       " ('exploring', 3),\n",
       " ('collaborate', 3),\n",
       " ('producing', 3),\n",
       " ('arising', 3),\n",
       " ('safely', 3),\n",
       " ('job', 3),\n",
       " ('legislation', 3),\n",
       " ('domain', 3),\n",
       " ('12', 3),\n",
       " ('Monash', 3),\n",
       " ('mechanics', 3),\n",
       " ('contact', 3),\n",
       " ('commercial', 3),\n",
       " ('matrix', 3),\n",
       " ('formulation', 3),\n",
       " (\"today's\", 3),\n",
       " ('foster', 3),\n",
       " ('examining', 3),\n",
       " ('Honours', 3),\n",
       " ('supervision', 3),\n",
       " ('benefits', 3),\n",
       " ('storage', 3),\n",
       " ('efficiency', 3),\n",
       " ('quantify', 3),\n",
       " ('landscape', 3),\n",
       " ('detail', 3),\n",
       " ('thermal', 3),\n",
       " ('phenomena', 3),\n",
       " ('diffusion', 3),\n",
       " ('sciences', 3),\n",
       " ('pressure', 3),\n",
       " ('cardiovascular', 3),\n",
       " ('Asia', 3),\n",
       " ('globalization', 3),\n",
       " ('lived', 3),\n",
       " ('nations', 3),\n",
       " ('continued', 3),\n",
       " ('age', 3),\n",
       " ('religious', 3),\n",
       " ('War', 3),\n",
       " ('shape', 3),\n",
       " ('protocols', 3),\n",
       " ('sets', 3),\n",
       " ('hospital', 3),\n",
       " ('differential', 3),\n",
       " ('ongoing', 3),\n",
       " ('analytic', 3),\n",
       " ('ecology', 3),\n",
       " ('full', 3),\n",
       " ('Implement', 3),\n",
       " ('determine', 3),\n",
       " ('peer', 3),\n",
       " ('hierarchy', 3),\n",
       " ('Explore', 3),\n",
       " ('communicative', 3),\n",
       " ('space', 3),\n",
       " ('image', 3),\n",
       " ('Work', 3),\n",
       " ('merits', 3),\n",
       " ('assumptions', 3),\n",
       " ('uncertainty', 3),\n",
       " ('company', 3),\n",
       " ('applicable', 3),\n",
       " ('histories', 3),\n",
       " ('overseas', 3),\n",
       " ('focussing', 3),\n",
       " ('record', 3),\n",
       " ('links', 3),\n",
       " ('papers', 3),\n",
       " ('visits', 3),\n",
       " ('urban', 3),\n",
       " ('in-depth', 3),\n",
       " ('asset', 3),\n",
       " ('pricing', 3),\n",
       " ('hands-on', 3),\n",
       " ('European', 3),\n",
       " ('show', 3),\n",
       " ('colonial', 3),\n",
       " ('source', 3),\n",
       " ('identities', 3),\n",
       " ('terrorism', 3),\n",
       " ('crime', 3),\n",
       " ('responding', 3),\n",
       " ('criminal', 3),\n",
       " ('OHS', 3),\n",
       " ('synthesize', 3),\n",
       " ('undertaking', 3),\n",
       " ('assisted', 3),\n",
       " ('experimentation', 3),\n",
       " ('situation', 3),\n",
       " ('regulations', 3),\n",
       " ('number', 3),\n",
       " ('regular', 3),\n",
       " ('monitoring', 3),\n",
       " ('weaknesses', 3),\n",
       " ('epidemiology', 3),\n",
       " ('childhood', 3),\n",
       " ('rights', 3),\n",
       " ('occur', 3),\n",
       " ('functional', 3),\n",
       " ('improve', 3),\n",
       " ('child', 3),\n",
       " ('influences', 3),\n",
       " ('classroom', 3),\n",
       " ('independently', 3),\n",
       " ('mathematics', 3),\n",
       " ('leading', 3),\n",
       " ('attitudes', 3),\n",
       " ('defined', 3),\n",
       " ('coherent', 3),\n",
       " ('covering', 3),\n",
       " ('special', 3),\n",
       " ('Indonesia', 3),\n",
       " ('reflects', 3),\n",
       " ('improved', 3),\n",
       " ('progress', 3),\n",
       " ('discovery', 3),\n",
       " ('reflection', 3),\n",
       " ('alternative', 3),\n",
       " ('peoples', 3),\n",
       " ('emergence', 3),\n",
       " ('interpretations', 3),\n",
       " ('IT', 3),\n",
       " ('involve', 3),\n",
       " ('innovation', 3),\n",
       " ('observational', 3),\n",
       " ('teams', 3),\n",
       " ('simulation', 3),\n",
       " ('stages', 3),\n",
       " ('combination', 3),\n",
       " ('simulated', 3),\n",
       " ('ensure', 3),\n",
       " ('detailed', 3),\n",
       " ('give', 3),\n",
       " ('correctly', 3),\n",
       " ('expanding', 3),\n",
       " ('demonstrates', 3),\n",
       " ('multicultural', 3),\n",
       " ('popular', 3),\n",
       " ('Indigenous', 3),\n",
       " ('interdisciplinary', 3),\n",
       " ('continue', 3),\n",
       " ('investment', 3),\n",
       " ('reaction', 3),\n",
       " ('operate', 3),\n",
       " ('reactions', 3),\n",
       " ('sustainable', 3),\n",
       " ('transition', 3),\n",
       " ('disorder', 3),\n",
       " ('workers', 3),\n",
       " ('successfully', 3),\n",
       " ('factual', 3),\n",
       " ('contribute', 3),\n",
       " ('definitions', 3),\n",
       " ('physiological', 3),\n",
       " ('interactive', 3),\n",
       " ('anatomy', 3),\n",
       " ('preparation', 3),\n",
       " ('philosophical', 3),\n",
       " ('parts', 3),\n",
       " ('communicating', 3),\n",
       " ('real-world', 3),\n",
       " ('consumer', 3),\n",
       " ('promotion', 3),\n",
       " ('scenario', 3),\n",
       " ('Skills', 3),\n",
       " ('section', 3),\n",
       " ('taught', 3),\n",
       " ('journalism', 3),\n",
       " ('publication', 3),\n",
       " ('activity', 3),\n",
       " ('forums', 3),\n",
       " ('diseases', 3),\n",
       " ('wider', 3),\n",
       " ('grounded', 3),\n",
       " ('justify', 3),\n",
       " ('practise', 3),\n",
       " ('grammar', 3),\n",
       " ...]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_freq.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of undesired tokens based on DF: 3770\n"
     ]
    }
   ],
   "source": [
    "df_high = 200*0.95\n",
    "df_low = 200*0.05\n",
    "undesired_df_token = [w for w,df in doc_freq.most_common() if df < df_low or df > df_high]\n",
    "print(f'# of undesired tokens based on DF: {len(undesired_df_token)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_tokens_Df = {u_code:token_Filter(tokens, undesired_df_token) for u_code,tokens in unit_tokens_Sw.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size with accept df : 275\n",
      "Total # of tokens with accept df : 8681\n",
      "Lexical diversity: 31.567272727272726\n"
     ]
    }
   ],
   "source": [
    "corpus_tokens_Df = list(chain.from_iterable(unit_tokens_Df.values()))\n",
    "print (f'Vocabulary size with accept df : {len(set(corpus_tokens_Df))}'\\\n",
    "       f'\\nTotal # of tokens with accept df : {len(corpus_tokens_Df)}'\\\n",
    "       f'\\nLexical diversity: {len(corpus_tokens_Df)/len(set(corpus_tokens_Df))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **<font color=blue>Too-short Tokens Removal </font>**\n",
    "> * We only keep the tokens whose length is equal to or greater than 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_tokens_gt3 = {u_code: [w for w in tokens if len(w)>=3] for u_code,tokens in unit_tokens_Df.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size with accept length : 273\n",
      "Total # of tokens with accept length: 8659\n",
      "Lexical diversity: 31.71794871794872\n"
     ]
    }
   ],
   "source": [
    "corpus_tokens_gt3 = list(chain.from_iterable(unit_tokens_gt3.values()))\n",
    "print (f'Vocabulary size with accept length : {len(set(corpus_tokens_gt3))}'\\\n",
    "       f'\\nTotal # of tokens with accept length: {len(corpus_tokens_gt3)}'\\\n",
    "       f'\\nLexical diversity: {len(corpus_tokens_gt3)/len(set(corpus_tokens_gt3))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Stemmer porter to stemming\n",
    "> * In the final stage, we use **PorterStemmer** for stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "unit_tokens_stemmed = {u_code: [stemmer.stem(w) if w[0].islower() else w for w in tokens] \\\n",
    "                       for u_code,tokens in unit_tokens_gt3.items()} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final vocabulary size: 217\n",
      "Total # of tokens: 8659\n",
      "Lexical diversity: 39.903225806451616\n"
     ]
    }
   ],
   "source": [
    "corpus_tokens_final = list(chain.from_iterable(unit_tokens_stemmed.values()))\n",
    "print (f'Final vocabulary size: {len(set(corpus_tokens_final))}'\\\n",
    "       f'\\nTotal # of tokens: {len(corpus_tokens_final)}'\\\n",
    "       f'\\nLexical diversity: {len(corpus_tokens_final)/len(set(corpus_tokens_final))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tokens length is greater than 3.\n"
     ]
    }
   ],
   "source": [
    "# double check the length of final tokens\n",
    "if not [w for w in corpus_tokens_final if len(w)<3]:\n",
    "    print('All tokens length is greater than 3.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Write vocab.txt  and countVec.txt\n",
    "> * First, we can write our corpus vocab into `vocab.txt` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tokens_sorted = sorted(set(corpus_tokens_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_id = '30086434'\n",
    "with open(f'{my_id}_vocab.txt', 'w') as f:\n",
    "    for i,token in enumerate(corpus_tokens_sorted):\n",
    "        line = f'{token}:{i}\\n'\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Then, we apply **CountVectorizer** to generate the `countVec.txt` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_codes = []\n",
    "tokens = []\n",
    "for u_code, token in unit_tokens_stemmed.items():\n",
    "    u_codes.append(u_code)\n",
    "    txt = ' '.join(token)\n",
    "    tokens.append(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(197, 217)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer( analyzer = 'word', lowercase=False)\n",
    "count_vectors = count_vectorizer.fit_transform(tokens)\n",
    "count_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = count_vectorizer.get_feature_names()\n",
    "cx = count_vectors.tocoo() # return the coordinate representation of a sparse matrix\n",
    "dic_write = {u_codes[i]:[] for i in range(197)}\n",
    "for i,j,v in itertools.zip_longest(cx.row, cx.col, cx.data):\n",
    "    dic_write[u_codes[i]].append((corpus_tokens_sorted.index(vocab[j]),(v)))\n",
    "for v in dic_write.values():\n",
    "    v.sort(key=lambda k: k[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{my_id}countVec.txt', 'w') as f:\n",
    "    for k,v in dic_write.items():\n",
    "        line = f'{k},'\n",
    "        for token_pair in v:\n",
    "            line += f'{token_pair[0]}:{token_pair[1]},'\n",
    "        line += \"\\n\"\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "> What we have done in this task:\n",
    "1. We create a **logic map** to handle the pre-processing problems, break them into component parts.\n",
    "2. First, the pdf file is parsed to get the raw text data.\n",
    "3. Then, We use **NLTK** to play with human language data:\n",
    "\n",
    "> * Sentence segmentation is applied for the purpose of normalization;  \n",
    "> * We tokenize data by words;\n",
    "> * The bigrams are identified and followed by retokenization;\n",
    "> * We remove bad features based on stopwords and document frequency;\n",
    "> * Finally, stemming is used for reducing inflected words to their word stem.\n",
    "     \n",
    "> After all the steps above, we've prepared our raw text data for topic analysis or sentiment analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
